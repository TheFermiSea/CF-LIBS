"""
Bayesian inference for CF-LIBS analysis.

This module implements Bayesian forward modeling and inference for CF-LIBS,
including:
- JAX-compatible forward model with full physics (Saha-Boltzmann, Voigt, Stark)
- Physically motivated priors for plasma parameters
- Log-likelihood function with realistic noise model (Poisson + Gaussian)

The implementation is designed to work with NumPyro for MCMC sampling.

References:
- Tognoni et al., "CF-LIBS: State of the art" (2010)
- Ciucci et al., "New procedure for quantitative elemental analysis by LIBS" (1999)
"""

from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple, Any, Union
import numpy as np
from enum import Enum

from cflibs.core.constants import (
    SAHA_CONST_CM3,
    C_LIGHT,
    EV_TO_K,
    EV_TO_J,
    KB_EV,
)
from cflibs.core.logging_config import get_logger

logger = get_logger("inversion.bayesian")

try:
    import jax
    import jax.numpy as jnp
    from jax import jit

    HAS_JAX = True
    # Import gradient-stable Faddeeva from profiles
    from cflibs.radiation.profiles import _faddeeva_weideman_jax
except ImportError:
    HAS_JAX = False
    jnp = None
    jit = lambda f: f
    _faddeeva_weideman_jax = None

try:
    import numpyro
    import numpyro.distributions as dist
    from numpyro.infer import MCMC, NUTS, init_to_value, init_to_median, init_to_uniform

    HAS_NUMPYRO = True
except ImportError:
    HAS_NUMPYRO = False
    numpyro = None
    dist = None

try:
    import arviz as az

    HAS_ARVIZ = True
except ImportError:
    HAS_ARVIZ = False
    az = None

try:
    import dynesty
    from dynesty import NestedSampler as DynestyNestedSampler
    from dynesty import DynamicNestedSampler as DynestyDynamicSampler

    HAS_DYNESTY = True
except ImportError:
    HAS_DYNESTY = False
    dynesty = None


# Physical constants
H_PLANCK = 6.626e-34  # Planck constant [J·s]
M_PROTON = 1.6726219e-27  # Proton mass [kg]

# Standard atomic masses for fallback [amu]
STANDARD_MASSES = {
    "H": 1.008, "He": 4.003, "Li": 6.941, "Be": 9.012, "B": 10.81,
    "C": 12.01, "N": 14.01, "O": 16.00, "F": 19.00, "Ne": 20.18,
    "Na": 22.99, "Mg": 24.31, "Al": 26.98, "Si": 28.09, "P": 30.97,
    "S": 32.07, "Cl": 35.45, "Ar": 39.95, "K": 39.10, "Ca": 40.08,
    "Sc": 44.96, "Ti": 47.87, "V": 50.94, "Cr": 52.00, "Mn": 54.94,
    "Fe": 55.85, "Co": 58.93, "Ni": 58.69, "Cu": 63.55, "Zn": 65.38,
}


@dataclass
class AtomicDataArrays:
    """
    Atomic data stored as JAX arrays for efficient computation.

    All arrays are indexed by line number (n_lines,).
    """

    wavelength_nm: Any  # Line wavelengths [nm]
    aki: Any  # Einstein A coefficients [s^-1]
    ek_ev: Any  # Upper level energy [eV]
    gk: Any  # Upper level degeneracy
    ip_ev: Any  # Ionization potential of parent species [eV]
    ion_stage: Any  # Ionization stage (0=neutral, 1=singly ionized)
    element_idx: Any  # Element index
    stark_w: Any  # Stark width reference [nm]
    stark_alpha: Any  # Stark temperature exponent
    mass_amu: Any  # Atomic mass [amu]
    partition_coeffs: Any  # Partition function coefficients (n_elements, n_stages, 5)
    ionization_potentials: Any  # Ionization potentials (n_elements, n_stages)
    elements: List[str] = field(default_factory=list)


@dataclass
class NoiseParameters:
    """
    Noise model parameters for LIBS spectra.

    The noise model combines:
    - Poisson shot noise: sqrt(I) (signal-dependent)
    - Gaussian readout noise: constant (signal-independent)
    - Background noise: additive offset

    Attributes
    ----------
    readout_noise : float
        RMS readout noise in counts (default: 10)
    dark_current : float
        Dark current per pixel in counts (default: 1)
    gain : float
        Detector gain (counts/photon) (default: 1)
    """

    readout_noise: float = 10.0
    dark_current: float = 1.0
    gain: float = 1.0


@dataclass
class PriorConfig:
    """
    Configuration for Bayesian priors on plasma parameters.

    Attributes
    ----------
    T_eV_range : Tuple[float, float]
        Temperature range in eV (default: 0.5-3.0 eV, typical LIBS)
    log_ne_range : Tuple[float, float]
        Log10(electron density) range (default: 15-19, i.e., 10^15-10^19 cm^-3)
    concentration_alpha : float
        Dirichlet prior concentration parameter (default: 1.0, uniform on simplex)
    """

    T_eV_range: Tuple[float, float] = (0.5, 3.0)
    # Full LIBS density range now supported thanks to Weideman Faddeeva approximation
    # (branch-free implementation with stable gradients)
    log_ne_range: Tuple[float, float] = (15.0, 19.0)
    concentration_alpha: float = 1.0


class ConvergenceStatus(Enum):
    """MCMC convergence status."""

    CONVERGED = "converged"
    NOT_CONVERGED = "not_converged"
    WARNING = "warning"
    UNKNOWN = "unknown"


@dataclass
class MCMCResult:
    """
    Result container for MCMC sampling with convergence diagnostics.

    Stores posterior samples, summary statistics, and convergence diagnostics
    from Bayesian CF-LIBS inference.

    Attributes
    ----------
    samples : dict
        Posterior samples for each parameter {name: array}
    T_eV : ParameterSummary
        Temperature summary (mean, std, credible intervals)
    log_ne : ParameterSummary
        Log electron density summary
    concentrations : dict
        Concentration summaries {element: ParameterSummary}
    r_hat : dict
        Gelman-Rubin R-hat statistic for each parameter
    ess : dict
        Effective sample size for each parameter
    convergence_status : ConvergenceStatus
        Overall convergence assessment
    n_samples : int
        Number of posterior samples
    n_chains : int
        Number of MCMC chains
    n_warmup : int
        Number of warmup samples
    inference_data : Any
        ArviZ InferenceData object for plotting (if ArviZ available)
    """

    # Raw samples
    samples: Dict[str, np.ndarray]

    # Summary statistics
    T_eV_mean: float
    T_eV_std: float
    T_eV_q025: float  # 2.5% quantile (lower 95% CI)
    T_eV_q975: float  # 97.5% quantile (upper 95% CI)

    log_ne_mean: float
    log_ne_std: float
    log_ne_q025: float
    log_ne_q975: float

    concentrations_mean: Dict[str, float]
    concentrations_std: Dict[str, float]
    concentrations_q025: Dict[str, float]
    concentrations_q975: Dict[str, float]

    # Convergence diagnostics
    r_hat: Dict[str, float] = field(default_factory=dict)
    ess: Dict[str, float] = field(default_factory=dict)
    convergence_status: ConvergenceStatus = ConvergenceStatus.UNKNOWN

    # Metadata
    n_samples: int = 0
    n_chains: int = 1
    n_warmup: int = 0

    # ArviZ InferenceData (for plotting)
    inference_data: Any = None

    # Derived quantities
    @property
    def n_e_mean(self) -> float:
        """Mean electron density [cm^-3]."""
        return 10.0 ** self.log_ne_mean

    @property
    def T_K_mean(self) -> float:
        """Mean temperature [K]."""
        return self.T_eV_mean * EV_TO_K

    @property
    def is_converged(self) -> bool:
        """Check if MCMC has converged (R-hat < 1.01 for all parameters)."""
        return self.convergence_status == ConvergenceStatus.CONVERGED

    def summary_table(self) -> str:
        """
        Generate a publication-ready summary table.

        Returns
        -------
        str
            Formatted summary table
        """
        lines = [
            "=" * 70,
            "CF-LIBS Bayesian Inference Results",
            "=" * 70,
            f"Samples: {self.n_samples} | Chains: {self.n_chains} | Warmup: {self.n_warmup}",
            f"Convergence: {self.convergence_status.value}",
            "-" * 70,
            f"{'Parameter':<20} {'Mean':>12} {'Std':>12} {'95% CI':>20}",
            "-" * 70,
            f"{'T [eV]':<20} {self.T_eV_mean:>12.4f} {self.T_eV_std:>12.4f} "
            f"[{self.T_eV_q025:.4f}, {self.T_eV_q975:.4f}]",
            f"{'T [K]':<20} {self.T_K_mean:>12.0f} {self.T_eV_std * EV_TO_K:>12.0f} "
            f"[{self.T_eV_q025 * EV_TO_K:.0f}, {self.T_eV_q975 * EV_TO_K:.0f}]",
            f"{'log10(n_e)':<20} {self.log_ne_mean:>12.4f} {self.log_ne_std:>12.4f} "
            f"[{self.log_ne_q025:.4f}, {self.log_ne_q975:.4f}]",
            f"{'n_e [cm^-3]':<20} {self.n_e_mean:>12.2e}",
        ]

        lines.append("-" * 70)
        lines.append(f"{'Element':<20} {'Conc.':<12} {'Std':>12} {'95% CI':>20}")
        lines.append("-" * 70)

        for el in self.concentrations_mean:
            mean = self.concentrations_mean[el]
            std = self.concentrations_std[el]
            q025 = self.concentrations_q025.get(el, mean - 2 * std)
            q975 = self.concentrations_q975.get(el, mean + 2 * std)
            lines.append(
                f"{el:<20} {mean:>12.4f} {std:>12.4f} [{q025:.4f}, {q975:.4f}]"
            )

        if self.r_hat:
            lines.append("-" * 70)
            lines.append("Convergence Diagnostics:")
            for param, rhat in self.r_hat.items():
                ess_val = self.ess.get(param, float("nan"))
                status = "✓" if rhat < 1.01 else "✗"
                lines.append(f"  {param}: R-hat={rhat:.3f} {status}, ESS={ess_val:.0f}")

        lines.append("=" * 70)
        return "\n".join(lines)

    def correlation_matrix(self, include_concentrations: bool = True) -> Dict[str, Any]:
        """
        Compute correlation matrix between posterior parameters.

        Correlation analysis helps identify parameter degeneracies and
        understand how uncertainties are coupled (e.g., T-n_e correlation).

        Parameters
        ----------
        include_concentrations : bool
            Include concentration parameters in correlation (default: True)

        Returns
        -------
        dict
            Contains:
            - 'matrix': np.ndarray correlation matrix
            - 'labels': list of parameter names
            - 'T_log_ne_corr': float, specific T vs log_ne correlation
        """
        # Extract flattened samples
        T_samples = np.array(self.samples["T_eV"]).flatten()
        log_ne_samples = np.array(self.samples["log_ne"]).flatten()

        # Build parameter matrix
        param_names = ["T_eV", "log_ne"]
        param_data = [T_samples, log_ne_samples]

        if include_concentrations and "concentrations" in self.samples:
            conc_samples = np.array(self.samples["concentrations"])
            # Handle different shapes (chains vs no chains)
            if conc_samples.ndim == 3:
                # (n_chains, n_samples, n_elements) -> (n_samples_total, n_elements)
                conc_samples = conc_samples.reshape(-1, conc_samples.shape[-1])
            elif conc_samples.ndim == 2:
                # (n_samples, n_elements) - already correct
                pass

            # Add each concentration
            for i, el in enumerate(self.concentrations_mean.keys()):
                param_names.append(f"C_{el}")
                param_data.append(conc_samples[:, i])

        # Stack and compute correlation
        data_matrix = np.vstack(param_data).T  # (n_samples, n_params)
        corr_matrix = np.corrcoef(data_matrix.T)

        # Key correlation: T vs log_ne
        T_log_ne_corr = corr_matrix[0, 1]

        return {
            "matrix": corr_matrix,
            "labels": param_names,
            "T_log_ne_corr": float(T_log_ne_corr),
        }

    def correlation_table(self) -> str:
        """
        Generate a formatted correlation table.

        Returns
        -------
        str
            Formatted correlation matrix table
        """
        corr_data = self.correlation_matrix()
        matrix = corr_data["matrix"]
        labels = corr_data["labels"]

        lines = [
            "=" * 70,
            "Parameter Correlations",
            "=" * 70,
        ]

        # Header row
        header = f"{'':>12}" + "".join(f"{l:>10}" for l in labels)
        lines.append(header)
        lines.append("-" * len(header))

        # Data rows
        for i, label in enumerate(labels):
            row = f"{label:>12}"
            for j in range(len(labels)):
                val = matrix[i, j]
                if i == j:
                    row += f"{'1.000':>10}"
                else:
                    row += f"{val:>10.3f}"
            lines.append(row)

        lines.append("-" * 70)
        lines.append(f"T - log_ne correlation: {corr_data['T_log_ne_corr']:.3f}")
        lines.append("=" * 70)

        return "\n".join(lines)


@dataclass
class NestedSamplingResult:
    """
    Result container for nested sampling with model evidence.

    Nested sampling provides both posterior samples AND marginal likelihood
    (evidence) for model comparison. The evidence Z = P(data | model) is
    crucial for comparing different plasma models (e.g., single-T vs multi-T).

    Attributes
    ----------
    samples : dict
        Posterior samples for each parameter {name: array}
    weights : np.ndarray
        Sample weights (normalized to sum to 1)
    log_evidence : float
        Log marginal likelihood ln(Z)
    log_evidence_err : float
        Uncertainty in log evidence
    information : float
        Kullback-Leibler divergence (bits of information gained)
    T_eV_mean : float
        Mean temperature [eV]
    T_eV_std : float
        Standard deviation of temperature
    log_ne_mean : float
        Mean log10(electron density)
    log_ne_std : float
        Standard deviation of log10(n_e)
    concentrations_mean : dict
        Mean concentrations by element
    concentrations_std : dict
        Standard deviations by element
    n_live : int
        Number of live points used
    n_iterations : int
        Number of nested sampling iterations
    n_calls : int
        Total likelihood evaluations
    """

    # Posterior samples and weights
    samples: Dict[str, np.ndarray]
    weights: np.ndarray

    # Evidence (model comparison)
    log_evidence: float
    log_evidence_err: float
    information: float  # KL divergence H

    # Summary statistics (weighted)
    T_eV_mean: float
    T_eV_std: float
    log_ne_mean: float
    log_ne_std: float
    concentrations_mean: Dict[str, float]
    concentrations_std: Dict[str, float]

    # Sampling metadata
    n_live: int = 100
    n_iterations: int = 0
    n_calls: int = 0

    @property
    def n_e_mean(self) -> float:
        """Mean electron density [cm^-3]."""
        return 10.0 ** self.log_ne_mean

    @property
    def T_K_mean(self) -> float:
        """Mean temperature [K]."""
        return self.T_eV_mean * EV_TO_K

    @property
    def evidence(self) -> float:
        """Marginal likelihood Z = exp(log_evidence)."""
        return np.exp(self.log_evidence)

    @property
    def bayes_factor_vs(self) -> str:
        """
        Interpretation helper for Bayes factors.

        Returns interpretation guidance for log evidence differences.
        """
        return (
            "Bayes factor interpretation (Kass & Raftery 1995):\n"
            "  |Δln(Z)| < 1:    Not worth more than a bare mention\n"
            "  1 < |Δln(Z)| < 3:  Positive evidence\n"
            "  3 < |Δln(Z)| < 5:  Strong evidence\n"
            "  |Δln(Z)| > 5:      Very strong evidence"
        )

    def summary_table(self) -> str:
        """Generate a publication-ready summary table."""
        lines = [
            "=" * 70,
            "CF-LIBS Nested Sampling Results",
            "=" * 70,
            f"Live points: {self.n_live} | Iterations: {self.n_iterations} | "
            f"Likelihood calls: {self.n_calls}",
            "-" * 70,
            "MODEL EVIDENCE:",
            f"  ln(Z) = {self.log_evidence:.2f} ± {self.log_evidence_err:.2f}",
            f"  Information (H) = {self.information:.2f} nats",
            "-" * 70,
            f"{'Parameter':<20} {'Mean':>12} {'Std':>12}",
            "-" * 70,
            f"{'T [eV]':<20} {self.T_eV_mean:>12.4f} {self.T_eV_std:>12.4f}",
            f"{'T [K]':<20} {self.T_K_mean:>12.0f} {self.T_eV_std * EV_TO_K:>12.0f}",
            f"{'log10(n_e)':<20} {self.log_ne_mean:>12.4f} {self.log_ne_std:>12.4f}",
            f"{'n_e [cm^-3]':<20} {self.n_e_mean:>12.2e}",
        ]

        lines.append("-" * 70)
        lines.append(f"{'Element':<20} {'Conc.':<12} {'Std':>12}")
        lines.append("-" * 70)

        for el in self.concentrations_mean:
            mean = self.concentrations_mean[el]
            std = self.concentrations_std[el]
            lines.append(f"{el:<20} {mean:>12.4f} {std:>12.4f}")

        lines.append("=" * 70)
        return "\n".join(lines)

    @staticmethod
    def compare_models(
        result_a: "NestedSamplingResult",
        result_b: "NestedSamplingResult",
        name_a: str = "Model A",
        name_b: str = "Model B",
    ) -> str:
        """
        Compare two models using Bayes factor.

        Parameters
        ----------
        result_a, result_b : NestedSamplingResult
            Results from two different models
        name_a, name_b : str
            Model names for display

        Returns
        -------
        str
            Formatted comparison with interpretation
        """
        delta_ln_z = result_a.log_evidence - result_b.log_evidence
        err = np.sqrt(result_a.log_evidence_err**2 + result_b.log_evidence_err**2)

        if abs(delta_ln_z) < 1:
            interpretation = "No significant preference"
        elif abs(delta_ln_z) < 3:
            preferred = name_a if delta_ln_z > 0 else name_b
            interpretation = f"Weak evidence for {preferred}"
        elif abs(delta_ln_z) < 5:
            preferred = name_a if delta_ln_z > 0 else name_b
            interpretation = f"Strong evidence for {preferred}"
        else:
            preferred = name_a if delta_ln_z > 0 else name_b
            interpretation = f"Very strong evidence for {preferred}"

        lines = [
            "=" * 60,
            "Bayesian Model Comparison",
            "=" * 60,
            f"{name_a}: ln(Z) = {result_a.log_evidence:.2f} ± {result_a.log_evidence_err:.2f}",
            f"{name_b}: ln(Z) = {result_b.log_evidence:.2f} ± {result_b.log_evidence_err:.2f}",
            "-" * 60,
            f"Δln(Z) = {delta_ln_z:.2f} ± {err:.2f}",
            f"Bayes factor K = {np.exp(delta_ln_z):.2e}",
            "-" * 60,
            f"Interpretation: {interpretation}",
            "=" * 60,
        ]
        return "\n".join(lines)


class BayesianForwardModel:
    """
    Bayesian forward model for CF-LIBS spectra.

    This class provides a JAX-compatible forward model that maps plasma
    parameters (T, n_e, concentrations) to synthetic spectra. The physics
    includes:
    - Saha-Boltzmann population distribution
    - Voigt line profiles (Weideman rational Faddeeva approximation)
    - Stark broadening with temperature scaling
    - Proper Doppler broadening with mass dependence

    Parameters
    ----------
    db_path : str
        Path to atomic database
    elements : List[str]
        Elements to include
    wavelength_range : Tuple[float, float]
        Wavelength range [nm]
    wavelength_grid : np.ndarray, optional
        Custom wavelength grid; if None, auto-generated
    instrument_fwhm_nm : float
        Instrument FWHM in nm (default: 0.05)
    """

    def __init__(
        self,
        db_path: str,
        elements: List[str],
        wavelength_range: Tuple[float, float],
        wavelength_grid: Optional[np.ndarray] = None,
        pixels: int = 2048,
        instrument_fwhm_nm: float = 0.05,
    ):
        if not HAS_JAX:
            raise ImportError("JAX required. Install with: pip install jax jaxlib")

        self.elements = elements
        self.wavelength_range = wavelength_range
        self.instrument_fwhm_nm = instrument_fwhm_nm

        # Create wavelength grid
        if wavelength_grid is not None:
            self.wavelength = jnp.array(wavelength_grid)
        else:
            self.wavelength = jnp.linspace(
                wavelength_range[0], wavelength_range[1], pixels
            )

        # Load atomic data
        self.atomic_data = self._load_atomic_data(db_path)

        logger.info(
            f"BayesianForwardModel: {len(elements)} elements, "
            f"{len(self.wavelength)} wavelengths, "
            f"{len(self.atomic_data.wavelength_nm)} lines"
        )

    def _load_atomic_data(self, db_path: str) -> AtomicDataArrays:
        """Load atomic data from database into JAX arrays."""
        import pandas as pd
        import sqlite3

        # Open direct connection for data loading (avoid AtomicDatabase to skip migrations)
        conn = sqlite3.connect(db_path)

        # Query spectral lines
        placeholders = ",".join(["?"] * len(self.elements))
        query = f"""
            SELECT
                l.element, l.sp_num, l.wavelength_nm, l.aki, l.ek_ev, l.gk,
                sp.ip_ev, l.stark_w, l.stark_alpha
            FROM lines l
            JOIN species_physics sp ON l.element = sp.element AND l.sp_num = sp.sp_num
            WHERE l.wavelength_nm BETWEEN ? AND ?
            AND l.element IN ({placeholders})
            ORDER BY l.wavelength_nm
        """
        params = [self.wavelength_range[0], self.wavelength_range[1]] + self.elements
        df = pd.read_sql_query(query, conn, params=params)

        if df.empty:
            raise ValueError(
                f"No atomic data for elements {self.elements} in "
                f"range {self.wavelength_range}"
            )

        # Map elements to indices
        el_map = {el: i for i, el in enumerate(self.elements)}
        df["el_idx"] = df["element"].map(el_map)

        # Get atomic masses from standard table (avoid AtomicDatabase instantiation)
        element_masses = {}
        for el in self.elements:
            if el in STANDARD_MASSES:
                element_masses[el] = STANDARD_MASSES[el]
            else:
                element_masses[el] = 50.0
                logger.warning(f"No mass for {el}, using fallback 50 amu")
        df["mass_amu"] = df["element"].map(element_masses)

        # Load partition function coefficients
        max_stages = 3
        n_elements = len(self.elements)
        coeffs = np.zeros((n_elements, max_stages, 5), dtype=np.float32)
        ips = np.zeros((n_elements, max_stages), dtype=np.float32)

        # Default coefficients
        coeffs[:, 0, 0] = np.log(25.0)
        coeffs[:, 1, 0] = np.log(15.0)
        coeffs[:, 2, 0] = np.log(10.0)

        try:
            cursor = conn.cursor()

            # Load ionization potentials
            cursor.execute(
                f"SELECT element, sp_num, ip_ev FROM species_physics "
                f"WHERE element IN ({placeholders})",
                self.elements,
            )
            for row in cursor.fetchall():
                el, sp_num, ip_ev = row
                if el in el_map and ip_ev is not None:
                    el_idx = el_map[el]
                    stage_idx = sp_num - 1
                    if 0 <= stage_idx < max_stages:
                        ips[el_idx, stage_idx] = ip_ev

            # Load partition function coefficients
            cursor.execute(
                "SELECT name FROM sqlite_master WHERE type='table' "
                "AND name='partition_functions'"
            )
            if cursor.fetchone():
                cursor.execute(
                    f"SELECT element, sp_num, a0, a1, a2, a3, a4 "
                    f"FROM partition_functions WHERE element IN ({placeholders})",
                    self.elements,
                )
                for row in cursor.fetchall():
                    el, sp_num, a0, a1, a2, a3, a4 = row
                    if el in el_map:
                        el_idx = el_map[el]
                        stage_idx = sp_num - 1
                        if 0 <= stage_idx < max_stages:
                            coeffs[el_idx, stage_idx] = [a0, a1, a2, a3, a4]
        except Exception as e:
            logger.warning(f"Failed to load physics data: {e}")
        finally:
            conn.close()

        # Handle missing Stark parameters
        stark_w_raw = df["stark_w"].fillna(float("nan")).values
        stark_alpha_raw = df["stark_alpha"].fillna(0.5).values

        return AtomicDataArrays(
            wavelength_nm=jnp.array(df["wavelength_nm"].values, dtype=jnp.float32),
            aki=jnp.array(df["aki"].values, dtype=jnp.float32),
            ek_ev=jnp.array(df["ek_ev"].values, dtype=jnp.float32),
            gk=jnp.array(df["gk"].values, dtype=jnp.float32),
            ip_ev=jnp.array(df["ip_ev"].values, dtype=jnp.float32),
            ion_stage=jnp.array(df["sp_num"].values - 1, dtype=jnp.int32),
            element_idx=jnp.array(df["el_idx"].values, dtype=jnp.int32),
            stark_w=jnp.array(stark_w_raw, dtype=jnp.float32),
            stark_alpha=jnp.array(stark_alpha_raw, dtype=jnp.float32),
            mass_amu=jnp.array(df["mass_amu"].values, dtype=jnp.float32),
            partition_coeffs=jnp.array(coeffs, dtype=jnp.float32),
            ionization_potentials=jnp.array(ips, dtype=jnp.float32),
            elements=self.elements,
        )

    def forward(
        self,
        T_eV: float,
        log_ne: float,
        concentrations: jnp.ndarray,
    ) -> jnp.ndarray:
        """
        Compute synthetic spectrum for given plasma parameters.

        Parameters
        ----------
        T_eV : float
            Temperature in eV
        log_ne : float
            Log10 of electron density [cm^-3]
        concentrations : array
            Element concentrations (must sum to 1)

        Returns
        -------
        array
            Synthetic spectrum intensity
        """
        n_e = 10.0 ** log_ne
        return self._compute_spectrum(T_eV, n_e, concentrations)

    def forward_numpy(
        self,
        T_eV: float,
        log_ne: float,
        concentrations: np.ndarray,
    ) -> np.ndarray:
        """
        Compute synthetic spectrum using NumPy arrays (for dynesty compatibility).

        This is a wrapper around forward() that handles NumPy <-> JAX conversion.

        Parameters
        ----------
        T_eV : float
            Temperature in eV
        log_ne : float
            Log10 of electron density [cm^-3]
        concentrations : np.ndarray
            Element concentrations (must sum to 1)

        Returns
        -------
        np.ndarray
            Synthetic spectrum intensity
        """
        conc_jax = jnp.array(concentrations)
        result = self.forward(T_eV, log_ne, conc_jax)
        return np.array(result)

    @staticmethod
    def _partition_function(T_K: float, coeffs: jnp.ndarray) -> jnp.ndarray:
        """
        Evaluate polynomial partition function.

        log(U) = sum_i a_i * (log(T))^i  (Irwin form)
        """
        log_T = jnp.log(T_K)
        powers = jnp.array([1.0, log_T, log_T**2, log_T**3, log_T**4])
        log_U = jnp.sum(coeffs * powers, axis=-1)
        return jnp.exp(log_U)

    def _compute_spectrum(
        self,
        T_eV: float,
        n_e: float,
        concentrations: jnp.ndarray,
    ) -> jnp.ndarray:
        """
        Compute spectrum with full physics.

        Uses Saha-Boltzmann populations, Voigt profiles, and Stark broadening.
        """
        data = self.atomic_data
        T_K = T_eV * EV_TO_K

        # Partition functions for all elements and stages
        U0 = self._partition_function(T_K, data.partition_coeffs[:, 0])
        U1 = self._partition_function(T_K, data.partition_coeffs[:, 1])

        # Ionization potentials for neutral -> ion transition
        IP_I = data.ionization_potentials[:, 0]

        # Saha ratio: n_ion / n_neutral
        saha_factor = (SAHA_CONST_CM3 / n_e) * (T_eV**1.5)
        ratio_ion_neutral = (
            2.0 * saha_factor * (U1 / U0) * jnp.exp(-IP_I / T_eV)
        )

        # Population fractions
        frac_neutral = 1.0 / (1.0 + ratio_ion_neutral)
        frac_ion = ratio_ion_neutral / (1.0 + ratio_ion_neutral)

        # Per-line quantities
        el_idx = data.element_idx
        ion_stage = data.ion_stage

        pop_fraction = jnp.where(ion_stage == 0, frac_neutral[el_idx], frac_ion[el_idx])
        U_val = jnp.where(ion_stage == 0, U0[el_idx], U1[el_idx])

        # Species number density
        element_conc = concentrations[el_idx]
        N_species_total = element_conc * n_e
        N_species = N_species_total * pop_fraction

        # Boltzmann upper level population
        n_upper = N_species * (data.gk / U_val) * jnp.exp(-data.ek_ev / T_eV)

        # Line emissivity: epsilon = (hc / 4pi * lambda) * A * n_upper
        epsilon = (H_PLANCK * C_LIGHT / (4 * jnp.pi * data.wavelength_nm * 1e-9)) * data.aki * n_upper

        # --- Line Broadening ---
        # Doppler width
        mass_kg = data.mass_amu * M_PROTON
        sigma_doppler = data.wavelength_nm * jnp.sqrt(
            2.0 * T_eV * EV_TO_J / (mass_kg * C_LIGHT**2)
        )

        # Instrument broadening
        sigma_inst = self.instrument_fwhm_nm / 2.355
        sigma_total = jnp.sqrt(sigma_doppler**2 + sigma_inst**2)

        # Stark broadening (HWHM)
        REF_NE = 1.0e16
        REF_T_EV = 0.86173  # 10000 K

        # Estimate Stark width for missing values
        binding_energy = jnp.maximum(data.ip_ev - data.ek_ev, 0.1)
        n_eff = (ion_stage + 1) * jnp.sqrt(13.605 / binding_energy)
        w_est = 2.0e-5 * (data.wavelength_nm / 500.0) ** 2 * (n_eff**4)
        w_est = jnp.clip(w_est, 0.0001, 0.5)
        w_ref = jnp.where(jnp.isnan(data.stark_w), w_est, data.stark_w)

        factor_ne = n_e / REF_NE
        factor_T = jnp.power(jnp.maximum(T_eV, 0.1) / REF_T_EV, -data.stark_alpha)
        gamma_stark = w_ref * factor_ne * factor_T

        # --- Voigt Profile (Weideman rational approximation) ---
        # Uses branch-free implementation for gradient stability during MCMC
        diff = self.wavelength[:, None] - data.wavelength_nm[None, :]
        z = (diff + 1j * gamma_stark) / (sigma_total * jnp.sqrt(2.0))

        # Gradient-stable Weideman approximation (no branching)
        w_z = _faddeeva_weideman_jax(z)

        profile = jnp.real(w_z) / (sigma_total * jnp.sqrt(2.0 * jnp.pi))

        # Sum line contributions
        intensity = jnp.sum(epsilon * profile, axis=1)

        # Clip to prevent numerical overflow at extreme parameters
        intensity = jnp.clip(intensity, 0.0, 1e12)

        return intensity


def log_likelihood(
    predicted: jnp.ndarray,
    observed: jnp.ndarray,
    noise_params: NoiseParameters = NoiseParameters(),
) -> float:
    """
    Compute log-likelihood for observed spectrum given predicted.

    The noise model combines Poisson shot noise and Gaussian readout noise:
        variance = predicted / gain + readout_noise^2 + dark_current

    This is the standard CCD noise model for spectroscopy.

    Parameters
    ----------
    predicted : array
        Predicted spectrum from forward model
    observed : array
        Observed spectrum (counts)
    noise_params : NoiseParameters
        Noise model parameters

    Returns
    -------
    float
        Log-likelihood value
    """
    # Ensure positive predicted values
    pred_safe = jnp.maximum(predicted, 1e-10)

    # Variance: Poisson (shot) + Gaussian (readout) + dark current
    variance = (
        pred_safe / noise_params.gain
        + noise_params.readout_noise**2
        + noise_params.dark_current
    )

    # Gaussian log-likelihood
    residual = observed - pred_safe
    log_lik = -0.5 * jnp.sum(
        jnp.log(2 * jnp.pi * variance) + residual**2 / variance
    )

    return log_lik


def bayesian_model(
    forward_model: BayesianForwardModel,
    observed: jnp.ndarray,
    prior_config: PriorConfig = PriorConfig(),
    noise_params: NoiseParameters = NoiseParameters(),
):
    """
    NumPyro probabilistic model for CF-LIBS Bayesian inference.

    This defines the full Bayesian model with priors and likelihood.
    Use with MCMC or variational inference.

    Parameters
    ----------
    forward_model : BayesianForwardModel
        Forward model instance
    observed : array
        Observed spectrum
    prior_config : PriorConfig
        Prior configuration
    noise_params : NoiseParameters
        Noise model parameters
    """
    if not HAS_NUMPYRO:
        raise ImportError("NumPyro required. Install with: pip install numpyro")

    n_elements = len(forward_model.elements)

    # --- Priors ---
    # Temperature: uniform on physically realistic range
    T_eV = numpyro.sample(
        "T_eV",
        dist.Uniform(prior_config.T_eV_range[0], prior_config.T_eV_range[1]),
    )

    # Electron density: log-uniform (Jeffreys prior for scale parameter)
    log_ne = numpyro.sample(
        "log_ne",
        dist.Uniform(prior_config.log_ne_range[0], prior_config.log_ne_range[1]),
    )

    # Concentrations: Dirichlet prior (ensures sum to 1)
    alpha = jnp.ones(n_elements) * prior_config.concentration_alpha
    concentrations = numpyro.sample("concentrations", dist.Dirichlet(alpha))

    # --- Forward Model ---
    predicted = forward_model.forward(T_eV, log_ne, concentrations)

    # --- Likelihood ---
    # Variance model: Poisson + readout noise
    # Add safeguards for numerical stability
    pred_safe = jnp.maximum(predicted, 1e-6)
    pred_safe = jnp.where(jnp.isnan(pred_safe), 1e-6, pred_safe)
    pred_safe = jnp.where(jnp.isinf(pred_safe), 1e6, pred_safe)

    variance = (
        pred_safe / noise_params.gain
        + noise_params.readout_noise**2
        + noise_params.dark_current
    )
    sigma = jnp.sqrt(jnp.maximum(variance, 1e-6))

    # Observe data
    numpyro.sample("obs", dist.Normal(pred_safe, sigma), obs=observed)


class MCMCSampler:
    """
    MCMC sampler for Bayesian CF-LIBS inference.

    Wraps NumPyro's NUTS sampler with sensible defaults for CF-LIBS,
    including convergence diagnostics, initialization strategies, and
    ArviZ integration for analysis and visualization.

    Parameters
    ----------
    forward_model : BayesianForwardModel
        Forward model instance
    prior_config : PriorConfig
        Prior configuration (default: PriorConfig())
    noise_params : NoiseParameters
        Noise model parameters (default: NoiseParameters())

    Example
    -------
    >>> sampler = MCMCSampler(forward_model)
    >>> result = sampler.run(observed_spectrum, num_samples=2000)
    >>> print(result.summary_table())
    >>> if result.is_converged:
    ...     print(f"T = {result.T_eV_mean:.3f} +/- {result.T_eV_std:.3f} eV")
    """

    def __init__(
        self,
        forward_model: BayesianForwardModel,
        prior_config: PriorConfig = PriorConfig(),
        noise_params: NoiseParameters = NoiseParameters(),
    ):
        if not HAS_NUMPYRO:
            raise ImportError("NumPyro required. Install with: pip install numpyro")

        self.forward_model = forward_model
        self.prior_config = prior_config
        self.noise_params = noise_params
        self.elements = forward_model.elements

        logger.info(
            f"MCMCSampler initialized: {len(self.elements)} elements, "
            f"T range={prior_config.T_eV_range} eV"
        )

    def _get_init_values(self) -> Dict[str, Any]:
        """
        Get sensible initial values for MCMC.

        Uses midpoints of prior ranges for temperature and density,
        and uniform concentrations.
        """
        n_elements = len(self.elements)

        # Midpoint of prior ranges
        T_init = (self.prior_config.T_eV_range[0] + self.prior_config.T_eV_range[1]) / 2
        log_ne_init = (
            self.prior_config.log_ne_range[0] + self.prior_config.log_ne_range[1]
        ) / 2

        # Uniform concentrations
        conc_init = jnp.ones(n_elements) / n_elements

        return {
            "T_eV": T_init,
            "log_ne": log_ne_init,
            "concentrations": conc_init,
        }

    def run(
        self,
        observed: np.ndarray,
        num_warmup: int = 500,
        num_samples: int = 1000,
        num_chains: int = 1,
        seed: int = 0,
        target_accept_prob: float = 0.8,
        max_tree_depth: int = 10,
        progress_bar: bool = True,
    ) -> MCMCResult:
        """
        Run MCMC sampling.

        Parameters
        ----------
        observed : array
            Observed spectrum
        num_warmup : int
            Number of warmup samples (default: 500)
        num_samples : int
            Number of posterior samples (default: 1000)
        num_chains : int
            Number of MCMC chains (default: 1, use 4 for production)
        seed : int
            Random seed
        target_accept_prob : float
            Target acceptance probability for NUTS (default: 0.8)
        max_tree_depth : int
            Maximum tree depth for NUTS (default: 10)
        progress_bar : bool
            Show progress bar (default: True)

        Returns
        -------
        MCMCResult
            Results with posterior samples and convergence diagnostics
        """
        import jax.random as random

        observed_jax = jnp.array(observed)
        n_elements = len(self.elements)

        # Get initial values
        init_values = self._get_init_values()

        # Create model function
        def model(obs):
            bayesian_model(
                self.forward_model, obs, self.prior_config, self.noise_params
            )

        # Create NUTS sampler with initialization
        # Use init_to_uniform for robustness - samples from prior to find valid start
        kernel = NUTS(
            model,
            init_strategy=init_to_uniform(radius=0.5),
            target_accept_prob=target_accept_prob,
            max_tree_depth=max_tree_depth,
        )

        mcmc = MCMC(
            kernel,
            num_warmup=num_warmup,
            num_samples=num_samples,
            num_chains=num_chains,
            progress_bar=progress_bar,
        )

        # Run sampling
        rng_key = random.PRNGKey(seed)
        logger.info(f"Starting MCMC: {num_chains} chains, {num_warmup} warmup, {num_samples} samples")

        mcmc.run(rng_key, observed_jax)

        # Get samples
        samples = mcmc.get_samples(group_by_chain=(num_chains > 1))

        # Process samples for single vs multi-chain
        if num_chains > 1:
            # Shape: (n_chains, n_samples, ...)
            T_samples = samples["T_eV"]
            log_ne_samples = samples["log_ne"]
            conc_samples = samples["concentrations"]
        else:
            # Shape: (n_samples, ...)
            T_samples = samples["T_eV"]
            log_ne_samples = samples["log_ne"]
            conc_samples = samples["concentrations"]

        # Flatten for statistics
        T_flat = np.array(T_samples).flatten()
        log_ne_flat = np.array(log_ne_samples).flatten()
        conc_flat = np.array(conc_samples).reshape(-1, n_elements)

        # Compute convergence diagnostics
        r_hat, ess = self._compute_convergence_diagnostics(mcmc, num_chains)

        # Determine convergence status
        convergence_status = self._assess_convergence(r_hat, ess, num_samples)

        # Build MCMCResult
        result = MCMCResult(
            samples={k: np.array(v) for k, v in samples.items()},
            T_eV_mean=float(np.mean(T_flat)),
            T_eV_std=float(np.std(T_flat)),
            T_eV_q025=float(np.percentile(T_flat, 2.5)),
            T_eV_q975=float(np.percentile(T_flat, 97.5)),
            log_ne_mean=float(np.mean(log_ne_flat)),
            log_ne_std=float(np.std(log_ne_flat)),
            log_ne_q025=float(np.percentile(log_ne_flat, 2.5)),
            log_ne_q975=float(np.percentile(log_ne_flat, 97.5)),
            concentrations_mean={
                el: float(np.mean(conc_flat[:, i]))
                for i, el in enumerate(self.elements)
            },
            concentrations_std={
                el: float(np.std(conc_flat[:, i]))
                for i, el in enumerate(self.elements)
            },
            concentrations_q025={
                el: float(np.percentile(conc_flat[:, i], 2.5))
                for i, el in enumerate(self.elements)
            },
            concentrations_q975={
                el: float(np.percentile(conc_flat[:, i], 97.5))
                for i, el in enumerate(self.elements)
            },
            r_hat=r_hat,
            ess=ess,
            convergence_status=convergence_status,
            n_samples=num_samples,
            n_chains=num_chains,
            n_warmup=num_warmup,
            inference_data=self._to_arviz(mcmc, num_chains) if HAS_ARVIZ else None,
        )

        logger.info(
            f"MCMC complete: T = {result.T_eV_mean:.3f} +/- {result.T_eV_std:.3f} eV, "
            f"n_e = {result.n_e_mean:.2e} cm^-3, "
            f"convergence={convergence_status.value}"
        )

        return result

    def _compute_convergence_diagnostics(
        self, mcmc: Any, num_chains: int
    ) -> Tuple[Dict[str, float], Dict[str, float]]:
        """
        Compute R-hat and ESS convergence diagnostics.

        Uses ArviZ if available, otherwise falls back to simple estimates.
        """
        r_hat = {}
        ess = {}

        if HAS_ARVIZ and num_chains > 1:
            # Use ArviZ for multi-chain diagnostics
            try:
                idata = az.from_numpyro(mcmc)

                # R-hat (should be < 1.01 for convergence)
                rhat_data = az.rhat(idata)
                for var in ["T_eV", "log_ne"]:
                    if var in rhat_data:
                        val = float(rhat_data[var].values)
                        r_hat[var] = val

                # ESS (effective sample size)
                ess_data = az.ess(idata)
                for var in ["T_eV", "log_ne"]:
                    if var in ess_data:
                        val = float(ess_data[var].values)
                        ess[var] = val

            except Exception as e:
                logger.warning(f"ArviZ diagnostics failed: {e}")
                r_hat, ess = self._simple_diagnostics(mcmc, num_chains)
        else:
            # Single chain or no ArviZ - use simple estimates
            r_hat, ess = self._simple_diagnostics(mcmc, num_chains)

        return r_hat, ess

    def _simple_diagnostics(
        self, mcmc: Any, num_chains: int
    ) -> Tuple[Dict[str, float], Dict[str, float]]:
        """Simple fallback diagnostics when ArviZ unavailable."""
        samples = mcmc.get_samples()
        r_hat = {}
        ess = {}

        for var in ["T_eV", "log_ne"]:
            if var in samples:
                s = np.array(samples[var]).flatten()
                # Simple ESS estimate using autocorrelation
                n = len(s)
                ess[var] = float(n)  # Naive - assume all samples independent

                # R-hat = 1 for single chain (can't compute between-chain variance)
                r_hat[var] = 1.0

        return r_hat, ess

    def _assess_convergence(
        self, r_hat: Dict[str, float], ess: Dict[str, float], num_samples: int
    ) -> ConvergenceStatus:
        """
        Assess overall convergence based on R-hat and ESS.

        Criteria:
        - CONVERGED: R-hat < 1.01 for all params, ESS > 100 for all params
        - WARNING: R-hat < 1.1, ESS > 50
        - NOT_CONVERGED: R-hat >= 1.1 or ESS < 50
        """
        if not r_hat or not ess:
            return ConvergenceStatus.UNKNOWN

        max_rhat = max(r_hat.values()) if r_hat else 1.0
        min_ess = min(ess.values()) if ess else num_samples

        if max_rhat < 1.01 and min_ess > 100:
            return ConvergenceStatus.CONVERGED
        elif max_rhat < 1.1 and min_ess > 50:
            return ConvergenceStatus.WARNING
        else:
            return ConvergenceStatus.NOT_CONVERGED

    def _to_arviz(self, mcmc: Any, num_chains: int) -> Any:
        """Convert MCMC results to ArviZ InferenceData."""
        if not HAS_ARVIZ:
            return None

        try:
            return az.from_numpyro(mcmc)
        except Exception as e:
            logger.warning(f"ArviZ conversion failed: {e}")
            return None

    def plot_trace(self, result: MCMCResult, figsize: Tuple[int, int] = (12, 8)) -> Any:
        """
        Generate trace plot using ArviZ.

        Parameters
        ----------
        result : MCMCResult
            MCMC result from run()
        figsize : tuple
            Figure size

        Returns
        -------
        matplotlib axes or None
        """
        if not HAS_ARVIZ:
            logger.warning("ArviZ required for plotting")
            return None

        if result.inference_data is None:
            logger.warning("No InferenceData available for plotting")
            return None

        try:
            return az.plot_trace(
                result.inference_data,
                var_names=["T_eV", "log_ne"],
                figsize=figsize,
            )
        except Exception as e:
            logger.warning(f"Trace plot failed: {e}")
            return None

    def plot_posterior(
        self, result: MCMCResult, figsize: Tuple[int, int] = (12, 6)
    ) -> Any:
        """
        Generate posterior distribution plot using ArviZ.

        Parameters
        ----------
        result : MCMCResult
            MCMC result from run()
        figsize : tuple
            Figure size

        Returns
        -------
        matplotlib axes or None
        """
        if not HAS_ARVIZ:
            logger.warning("ArviZ required for plotting")
            return None

        if result.inference_data is None:
            logger.warning("No InferenceData available for plotting")
            return None

        try:
            return az.plot_posterior(
                result.inference_data,
                var_names=["T_eV", "log_ne"],
                figsize=figsize,
                hdi_prob=0.95,
            )
        except Exception as e:
            logger.warning(f"Posterior plot failed: {e}")
            return None

    def plot_corner(
        self,
        result: MCMCResult,
        var_names: Optional[List[str]] = None,
        figsize: Tuple[int, int] = (10, 10),
        show_titles: bool = True,
    ) -> Any:
        """
        Generate corner/pair plot showing parameter correlations.

        Corner plots show 2D marginal distributions and correlations between
        all pairs of parameters, essential for understanding degeneracies.

        Parameters
        ----------
        result : MCMCResult
            MCMC result from run()
        var_names : list, optional
            Parameters to include (default: ["T_eV", "log_ne"])
        figsize : tuple
            Figure size
        show_titles : bool
            Show parameter summaries in titles

        Returns
        -------
        matplotlib axes or None
        """
        if not HAS_ARVIZ:
            logger.warning("ArviZ required for corner plots")
            return None

        if result.inference_data is None:
            logger.warning("No InferenceData available for plotting")
            return None

        if var_names is None:
            var_names = ["T_eV", "log_ne"]

        try:
            return az.plot_pair(
                result.inference_data,
                var_names=var_names,
                kind="kde",
                marginals=True,
                figsize=figsize,
                textsize=10,
            )
        except Exception as e:
            logger.warning(f"Corner plot failed: {e}")
            return None

    def plot_forest(
        self, result: MCMCResult, figsize: Tuple[int, int] = (10, 6)
    ) -> Any:
        """
        Generate forest plot comparing parameter estimates with credible intervals.

        Forest plots are useful for comparing multiple parameters or results
        from different runs.

        Parameters
        ----------
        result : MCMCResult
            MCMC result from run()
        figsize : tuple
            Figure size

        Returns
        -------
        matplotlib axes or None
        """
        if not HAS_ARVIZ:
            logger.warning("ArviZ required for forest plots")
            return None

        if result.inference_data is None:
            logger.warning("No InferenceData available for plotting")
            return None

        try:
            return az.plot_forest(
                result.inference_data,
                var_names=["T_eV", "log_ne"],
                combined=True,
                figsize=figsize,
                hdi_prob=0.95,
            )
        except Exception as e:
            logger.warning(f"Forest plot failed: {e}")
            return None


class NestedSampler:
    """
    Nested sampler for Bayesian CF-LIBS inference with model comparison.

    Nested sampling provides two key advantages over MCMC:
    1. Direct evidence (marginal likelihood) calculation for model comparison
    2. Better handling of multimodal posteriors

    Uses dynesty for efficient nested sampling with dynamic allocation.

    Parameters
    ----------
    forward_model : BayesianForwardModel
        Forward model instance
    prior_config : PriorConfig
        Prior configuration (default: PriorConfig())
    noise_params : NoiseParameters
        Noise model parameters (default: NoiseParameters())

    Example
    -------
    >>> sampler = NestedSampler(forward_model)
    >>> result = sampler.run(observed_spectrum, nlive=100)
    >>> print(f"Evidence: ln(Z) = {result.log_evidence:.2f}")
    >>> print(result.summary_table())

    Notes
    -----
    For model comparison, run nested sampling on both models and compare:
    >>> result_single_T = sampler_single.run(spectrum)
    >>> result_multi_T = sampler_multi.run(spectrum)
    >>> print(NestedSamplingResult.compare_models(result_single_T, result_multi_T))
    """

    def __init__(
        self,
        forward_model: BayesianForwardModel,
        prior_config: PriorConfig = PriorConfig(),
        noise_params: NoiseParameters = NoiseParameters(),
    ):
        if not HAS_DYNESTY:
            raise ImportError("dynesty required. Install with: pip install dynesty")

        self.forward_model = forward_model
        self.prior_config = prior_config
        self.noise_params = noise_params
        self.elements = forward_model.elements
        self.n_elements = len(self.elements)

        # Parameter dimension: T_eV + log_ne + (n_elements - 1) concentrations
        # Use n_elements - 1 because concentrations sum to 1 (simplex)
        self.ndim = 2 + (self.n_elements - 1)

        logger.info(
            f"NestedSampler initialized: {self.n_elements} elements, "
            f"{self.ndim} dimensions, T range={prior_config.T_eV_range} eV"
        )

    def _prior_transform(self, u: np.ndarray) -> np.ndarray:
        """
        Transform unit cube [0,1]^n to physical parameter space.

        dynesty samples from unit cube; we transform to physical priors.

        Parameters
        ----------
        u : array
            Samples from unit cube [0, 1]^ndim

        Returns
        -------
        array
            Physical parameters [T_eV, log_ne, c_1, ..., c_{n-1}]
        """
        x = np.zeros_like(u)

        # T_eV: uniform prior
        T_min, T_max = self.prior_config.T_eV_range
        x[0] = T_min + u[0] * (T_max - T_min)

        # log_ne: uniform prior (log-uniform in n_e)
        log_ne_min, log_ne_max = self.prior_config.log_ne_range
        x[1] = log_ne_min + u[1] * (log_ne_max - log_ne_min)

        # Concentrations: Dirichlet-like via stick-breaking
        # For n elements, we have n-1 free parameters
        if self.n_elements > 1:
            # Stick-breaking transformation for Dirichlet(alpha, alpha, ..., alpha)
            alpha = self.prior_config.concentration_alpha
            remaining = 1.0
            for i in range(self.n_elements - 1):
                # Beta distribution via inverse CDF
                from scipy import stats
                beta_sample = stats.beta.ppf(u[2 + i], alpha, alpha * (self.n_elements - 1 - i))
                x[2 + i] = remaining * beta_sample
                remaining -= x[2 + i]

        return x

    def _params_to_concentrations(self, params: np.ndarray) -> np.ndarray:
        """
        Convert parameter vector to full concentration array.

        The last concentration is determined by sum-to-one constraint.
        """
        if self.n_elements == 1:
            return np.array([1.0])

        conc = np.zeros(self.n_elements)
        conc[:-1] = params[2:]
        conc[-1] = max(0.0, 1.0 - np.sum(conc[:-1]))
        return conc

    def _log_likelihood(
        self, params: np.ndarray, observed: np.ndarray
    ) -> float:
        """
        Compute log-likelihood for nested sampling.

        Parameters
        ----------
        params : array
            [T_eV, log_ne, c_1, ..., c_{n-1}]
        observed : array
            Observed spectrum

        Returns
        -------
        float
            Log-likelihood value
        """
        T_eV = params[0]
        log_ne = params[1]
        concentrations = self._params_to_concentrations(params)

        # Check bounds
        if T_eV <= 0 or np.any(concentrations < 0) or np.any(concentrations > 1):
            return -np.inf

        try:
            # Forward model (using NumPy version for dynesty compatibility)
            predicted = self.forward_model.forward_numpy(T_eV, log_ne, concentrations)

            # Log-likelihood with noise model
            sigma_read = self.noise_params.readout_noise
            dark = self.noise_params.dark_current

            # Combined variance: shot noise + readout + dark
            variance = np.abs(predicted) + sigma_read**2 + dark

            # Gaussian log-likelihood
            residuals = observed - predicted
            log_lik = -0.5 * np.sum(residuals**2 / variance + np.log(2 * np.pi * variance))

            if not np.isfinite(log_lik):
                return -np.inf

            return float(log_lik)

        except Exception:
            return -np.inf

    def run(
        self,
        observed: np.ndarray,
        nlive: int = 100,
        dlogz: float = 0.1,
        sample: str = "auto",
        bound: str = "multi",
        seed: int = 42,
        maxiter: Optional[int] = None,
        maxcall: Optional[int] = None,
        verbose: bool = True,
    ) -> NestedSamplingResult:
        """
        Run nested sampling.

        Parameters
        ----------
        observed : array
            Observed spectrum
        nlive : int
            Number of live points (default: 100, use 500+ for production)
        dlogz : float
            Target evidence tolerance (default: 0.1)
        sample : str
            Sampling method: 'auto', 'unif', 'rwalk', 'slice', 'rslice'
        bound : str
            Bounding method: 'none', 'single', 'multi', 'balls', 'cubes'
        seed : int
            Random seed
        maxiter : int, optional
            Maximum iterations
        maxcall : int, optional
            Maximum likelihood calls
        verbose : bool
            Show progress (default: True)

        Returns
        -------
        NestedSamplingResult
            Results with posterior samples and evidence
        """
        observed_np = np.asarray(observed)

        # Create likelihood function (closure over observed)
        def loglike(params):
            return self._log_likelihood(params, observed_np)

        # Set random state
        rstate = np.random.default_rng(seed)

        logger.info(
            f"Starting nested sampling: nlive={nlive}, dlogz={dlogz}, "
            f"ndim={self.ndim}"
        )

        # Create and run sampler
        sampler = DynestyNestedSampler(
            loglike,
            self._prior_transform,
            self.ndim,
            nlive=nlive,
            bound=bound,
            sample=sample,
            rstate=rstate,
        )

        sampler.run_nested(
            dlogz=dlogz,
            maxiter=maxiter,
            maxcall=maxcall,
            print_progress=verbose,
        )

        results = sampler.results

        # Extract results
        samples = results.samples  # Shape: (n_samples, ndim)
        weights = np.exp(results.logwt - results.logwt.max())
        weights /= weights.sum()

        # Evidence
        log_evidence = float(results.logz[-1])
        log_evidence_err = float(results.logzerr[-1])
        information = float(results.information[-1]) if hasattr(results, 'information') else 0.0

        # Compute weighted statistics
        T_samples = samples[:, 0]
        log_ne_samples = samples[:, 1]

        T_mean = float(np.average(T_samples, weights=weights))
        T_std = float(np.sqrt(np.average((T_samples - T_mean)**2, weights=weights)))

        log_ne_mean = float(np.average(log_ne_samples, weights=weights))
        log_ne_std = float(np.sqrt(np.average((log_ne_samples - log_ne_mean)**2, weights=weights)))

        # Concentration statistics
        conc_samples = np.array([self._params_to_concentrations(s) for s in samples])
        conc_mean = {}
        conc_std = {}
        for i, el in enumerate(self.elements):
            c_samples = conc_samples[:, i]
            c_mean = float(np.average(c_samples, weights=weights))
            c_std = float(np.sqrt(np.average((c_samples - c_mean)**2, weights=weights)))
            conc_mean[el] = c_mean
            conc_std[el] = c_std

        # Build result
        result = NestedSamplingResult(
            samples={
                "T_eV": T_samples,
                "log_ne": log_ne_samples,
                "concentrations": conc_samples,
            },
            weights=weights,
            log_evidence=log_evidence,
            log_evidence_err=log_evidence_err,
            information=information,
            T_eV_mean=T_mean,
            T_eV_std=T_std,
            log_ne_mean=log_ne_mean,
            log_ne_std=log_ne_std,
            concentrations_mean=conc_mean,
            concentrations_std=conc_std,
            n_live=nlive,
            n_iterations=int(results.niter),
            n_calls=int(np.sum(results.ncall)),  # ncall may be array
        )

        logger.info(
            f"Nested sampling complete: ln(Z) = {log_evidence:.2f} ± {log_evidence_err:.2f}, "
            f"T = {T_mean:.3f} ± {T_std:.3f} eV"
        )

        return result


def run_mcmc(
    forward_model: BayesianForwardModel,
    observed: np.ndarray,
    prior_config: PriorConfig = PriorConfig(),
    noise_params: NoiseParameters = NoiseParameters(),
    num_warmup: int = 500,
    num_samples: int = 1000,
    num_chains: int = 1,
    seed: int = 0,
) -> Dict[str, Any]:
    """
    Run MCMC sampling for Bayesian CF-LIBS inference.

    This is a convenience wrapper around MCMCSampler for backwards compatibility.
    For full functionality including convergence diagnostics and ArviZ integration,
    use MCMCSampler directly.

    Parameters
    ----------
    forward_model : BayesianForwardModel
        Forward model instance
    observed : array
        Observed spectrum
    prior_config : PriorConfig
        Prior configuration
    noise_params : NoiseParameters
        Noise model parameters
    num_warmup : int
        Number of warmup samples
    num_samples : int
        Number of posterior samples
    num_chains : int
        Number of MCMC chains
    seed : int
        Random seed

    Returns
    -------
    dict
        MCMC results including posterior samples (legacy format)
    """
    sampler = MCMCSampler(forward_model, prior_config, noise_params)
    result = sampler.run(
        observed,
        num_warmup=num_warmup,
        num_samples=num_samples,
        num_chains=num_chains,
        seed=seed,
        progress_bar=False,
    )

    # Return legacy format for backwards compatibility
    return {
        "samples": result.samples,
        "T_eV_mean": result.T_eV_mean,
        "T_eV_std": result.T_eV_std,
        "log_ne_mean": result.log_ne_mean,
        "log_ne_std": result.log_ne_std,
        "concentrations_mean": result.concentrations_mean,
        "concentrations_std": result.concentrations_std,
        "n_e_mean": result.n_e_mean,
        "T_K_mean": result.T_K_mean,
    }


# --- Convenience functions for priors (CF-LIBS-zbs) ---


def create_temperature_prior(
    T_min_eV: float = 0.5,
    T_max_eV: float = 3.0,
    prior_type: str = "uniform",
) -> Any:
    """
    Create temperature prior distribution.

    Parameters
    ----------
    T_min_eV : float
        Minimum temperature [eV]
    T_max_eV : float
        Maximum temperature [eV]
    prior_type : str
        Prior type: 'uniform', 'normal', 'truncnorm'

    Returns
    -------
    numpyro.distribution
        Prior distribution
    """
    if not HAS_NUMPYRO:
        raise ImportError("NumPyro required")

    if prior_type == "uniform":
        return dist.Uniform(T_min_eV, T_max_eV)
    elif prior_type == "normal":
        # Centered on typical LIBS temperature
        mean = (T_min_eV + T_max_eV) / 2
        std = (T_max_eV - T_min_eV) / 4
        return dist.TruncatedNormal(mean, std, low=T_min_eV, high=T_max_eV)
    else:
        return dist.Uniform(T_min_eV, T_max_eV)


def create_density_prior(
    log_ne_min: float = 15.0,
    log_ne_max: float = 19.0,
    prior_type: str = "uniform",
) -> Any:
    """
    Create electron density prior distribution.

    Log-uniform (Jeffreys) prior is appropriate for scale parameters.

    Parameters
    ----------
    log_ne_min : float
        Log10 of minimum density [cm^-3]
    log_ne_max : float
        Log10 of maximum density [cm^-3]
    prior_type : str
        Prior type: 'uniform' (log-uniform), 'normal'

    Returns
    -------
    numpyro.distribution
        Prior distribution for log10(n_e)
    """
    if not HAS_NUMPYRO:
        raise ImportError("NumPyro required")

    if prior_type == "uniform":
        # Log-uniform = uniform on log scale = Jeffreys prior
        return dist.Uniform(log_ne_min, log_ne_max)
    elif prior_type == "normal":
        mean = (log_ne_min + log_ne_max) / 2
        std = (log_ne_max - log_ne_min) / 4
        return dist.TruncatedNormal(mean, std, low=log_ne_min, high=log_ne_max)
    else:
        return dist.Uniform(log_ne_min, log_ne_max)


def create_concentration_prior(
    n_elements: int,
    alpha: float = 1.0,
    known_concentrations: Optional[Dict[int, float]] = None,
) -> Any:
    """
    Create concentration prior distribution.

    Uses Dirichlet distribution which naturally enforces:
    - All concentrations positive
    - Concentrations sum to 1

    Parameters
    ----------
    n_elements : int
        Number of elements
    alpha : float
        Dirichlet concentration parameter:
        - alpha = 1: Uniform on simplex
        - alpha > 1: Peaked at center (equal concentrations)
        - alpha < 1: Peaked at corners (sparse compositions)
    known_concentrations : dict, optional
        Known concentration constraints {element_idx: value}

    Returns
    -------
    numpyro.distribution
        Prior distribution
    """
    if not HAS_NUMPYRO:
        raise ImportError("NumPyro required")

    alphas = jnp.ones(n_elements) * alpha

    # Adjust for known concentrations (informative prior)
    if known_concentrations:
        for idx, value in known_concentrations.items():
            # Increase alpha for known elements to peak near their value
            alphas = alphas.at[idx].set(alpha * (1 + 10 * value))

    return dist.Dirichlet(alphas)
